{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "176e4c93-1b61-491c-92af-be84a105574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ce4e6c-e12b-4a07-86d8-ce03fdd6efa8",
   "metadata": {},
   "source": [
    "![Original-ResNet-18-Architecture.png](Original-ResNet-18-Architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "700ac8cf-edd0-4b9d-9e11-f6ecab0ddc6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d.cherniuk/.conda/mark23/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/d.cherniuk/.conda/mark23/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_model = resnet18(pretrained=True)\n",
    "orig_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1065d8e8-4456-43c3-b4c9-e24175832a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_model.layer2[0].downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbd014fe-0fc2-4cb6-902b-00688a1ccdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: conv1\n",
      "bias: False\n",
      "Original Shape: torch.Size([64, 3, 7, 7])\n",
      "Flattened Shape: torch.Size([64, 3, 49])\n",
      "Input shape: torch.Size([1, 3, 224, 224])\n",
      "\n",
      "Parameters: 9408\n",
      "Original MACs: 118013952\n",
      "Reduction rates: [1, 1.5, 2]\n",
      "Reduction ranks: [81, 54, 40]\n",
      "\n",
      "Layer: layer1.0.conv1\n",
      "bias: False\n",
      "Original Shape: torch.Size([64, 64, 3, 3])\n",
      "Flattened Shape: torch.Size([64, 64, 9])\n",
      "Input shape: torch.Size([1, 64, 56, 56])\n",
      "Output shape: torch.Size([1, 64, 56, 56])\n",
      "Parameters: 36864\n",
      "Original MACs: 115605504\n",
      "Reduction rates: [1, 1.5, 2]\n",
      "Reduction ranks: [269, 179, 134]\n",
      "Reduced MACs(%): [99.97, 66.52, 49.8]\n",
      "\n",
      "Layer: layer1.0.conv2\n",
      "bias: False\n",
      "Original Shape: torch.Size([64, 64, 3, 3])\n",
      "Flattened Shape: torch.Size([64, 64, 9])\n",
      "Input shape: torch.Size([1, 64, 56, 56])\n",
      "Output shape: torch.Size([1, 64, 56, 56])\n",
      "Parameters: 36864\n",
      "Original MACs: 115605504\n",
      "Reduction rates: [1, 1.5, 2]\n",
      "Reduction ranks: [269, 179, 134]\n",
      "Reduced MACs(%): [99.97, 66.52, 49.8]\n",
      "\n",
      "Layer: layer1.1.conv1\n",
      "bias: False\n",
      "Original Shape: torch.Size([64, 64, 3, 3])\n",
      "Flattened Shape: torch.Size([64, 64, 9])\n",
      "Input shape: torch.Size([1, 64, 56, 56])\n",
      "Output shape: torch.Size([1, 64, 56, 56])\n",
      "Parameters: 36864\n",
      "Original MACs: 115605504\n",
      "Reduction rates: [1, 1.5, 2]\n",
      "Reduction ranks: [269, 179, 134]\n",
      "Reduced MACs(%): [99.97, 66.52, 49.8]\n",
      "\n",
      "Layer: layer1.1.conv2\n",
      "bias: False\n",
      "Original Shape: torch.Size([64, 64, 3, 3])\n",
      "Flattened Shape: torch.Size([64, 64, 9])\n",
      "Input shape: torch.Size([1, 64, 56, 56])\n",
      "Output shape: torch.Size([1, 64, 56, 56])\n",
      "Parameters: 36864\n",
      "Original MACs: 115605504\n",
      "Reduction rates: [1, 1.5, 2]\n",
      "Reduction ranks: [269, 179, 134]\n",
      "Reduced MACs(%): [99.97, 66.52, 49.8]\n",
      "\n",
      "Layer: layer2.0.conv1\n",
      "bias: False\n",
      "Original Shape: torch.Size([128, 64, 3, 3])\n",
      "Flattened Shape: torch.Size([128, 64, 9])\n",
      "Input shape: torch.Size([1, 64, 56, 56])\n",
      "Output shape: torch.Size([1, 128, 28, 28])\n",
      "Parameters: 73728\n",
      "Original MACs: 57802752\n",
      "Reduction rates: [1, 1.5, 2]\n",
      "Reduction ranks: [366, 244, 183]\n",
      "Reduced MACs(%): [195.09, 130.06, 97.55]\n",
      "\n",
      "Layer: layer2.0.conv2\n",
      "bias: False\n",
      "Original Shape: torch.Size([128, 128, 3, 3])\n",
      "Flattened Shape: torch.Size([128, 128, 9])\n",
      "Input shape: torch.Size([1, 128, 28, 28])\n",
      "Output shape: torch.Size([1, 128, 28, 28])\n",
      "Parameters: 147456\n",
      "Original MACs: 115605504\n",
      "Reduction rates: [1, 1.5, 2]\n",
      "Reduction ranks: [556, 370, 278]\n",
      "Reduced MACs(%): [99.92, 66.49, 49.96]\n",
      "\n",
      "Layer: layer2.0.downsample\n",
      "bias: False\n",
      "Original Shape: torch.Size([128, 64, 1, 1])\n",
      "Flattened Shape: torch.Size([128, 64])\n",
      "Input shape: torch.Size([1, 64, 56, 56])\n",
      "Output shape: torch.Size([1, 128, 28, 28])\n",
      "Parameters: 8192\n",
      "Original MACs: 6422528\n",
      "Reduction rates: [1, 1.5, 2]\n",
      "Reduction ranks: [42, 28, 21]\n",
      "\n",
      "Layer: layer2.1.conv1\n",
      "bias: False\n",
      "Original Shape: torch.Size([128, 128, 3, 3])\n",
      "Flattened Shape: torch.Size([128, 128, 9])\n",
      "Input shape: torch.Size([1, 128, 28, 28])\n",
      "Output shape: torch.Size([1, 128, 28, 28])\n",
      "Parameters: 147456\n",
      "Original MACs: 115605504\n",
      "Reduction rates: [1, 1.5, 2]\n",
      "Reduction ranks: [556, 370, 278]\n",
      "Reduced MACs(%): [99.92, 66.49, 49.96]\n",
      "\n",
      "Layer: layer2.1.conv2\n",
      "bias: False\n",
      "Original Shape: torch.Size([128, 128, 3, 3])\n",
      "Flattened Shape: torch.Size([128, 128, 9])\n",
      "Input shape: torch.Size([1, 128, 28, 28])\n",
      "Output shape: torch.Size([1, 128, 28, 28])\n",
      "Parameters: 147456\n",
      "Original MACs: 115605504\n",
      "Reduction rates: [1, 1.5, 2]\n",
      "Reduction ranks: [556, 370, 278]\n",
      "Reduced MACs(%): [99.92, 66.49, 49.96]\n",
      "\n",
      "Layer: layer3.0.conv1\n",
      "bias: False\n",
      "Original Shape: torch.Size([256, 128, 3, 3])\n",
      "Flattened Shape: torch.Size([256, 128, 9])\n",
      "Input shape: torch.Size([1, 128, 28, 28])\n",
      "Output shape: torch.Size([1, 256, 14, 14])\n",
      "Parameters: 294912\n",
      "Original MACs: 57802752\n",
      "Reduction rates: [1, 1.5, 2]\n",
      "Reduction ranks: [750, 500, 375]\n",
      "Reduced MACs(%): [197.6, 131.73, 98.8]\n",
      "\n",
      "Layer: layer3.0.conv2\n",
      "bias: False\n",
      "Original Shape: torch.Size([256, 256, 3, 3])\n",
      "Flattened Shape: torch.Size([256, 256, 9])\n",
      "Input shape: torch.Size([1, 256, 14, 14])\n",
      "Output shape: torch.Size([1, 256, 14, 14])\n",
      "Parameters: 589824\n",
      "Original MACs: 115605504\n",
      "Reduction rates: [1, 1.5, 2]\n",
      "Reduction ranks: [1132, 754, 566]\n",
      "Reduced MACs(%): [99.99, 66.6, 50.0]\n",
      "\n",
      "Layer: layer3.0.downsample\n",
      "bias: False\n",
      "Original Shape: torch.Size([256, 128, 1, 1])\n",
      "Flattened Shape: torch.Size([256, 128])\n",
      "Input shape: torch.Size([1, 128, 28, 28])\n",
      "Output shape: torch.Size([1, 256, 14, 14])\n",
      "Parameters: 32768\n",
      "Original MACs: 6422528\n",
      "Reduction rates: [1, 1.5, 2]\n",
      "Reduction ranks: [85, 56, 42]\n",
      "\n",
      "Layer: layer3.1.conv1\n",
      "bias: False\n",
      "Original Shape: torch.Size([256, 256, 3, 3])\n",
      "Flattened Shape: torch.Size([256, 256, 9])\n",
      "Input shape: torch.Size([1, 256, 14, 14])\n",
      "Output shape: torch.Size([1, 256, 14, 14])\n",
      "Parameters: 589824\n",
      "Original MACs: 115605504\n",
      "Reduction rates: [1, 1.5, 2]\n",
      "Reduction ranks: [1132, 754, 566]\n",
      "Reduced MACs(%): [99.99, 66.6, 50.0]\n",
      "\n",
      "Layer: layer3.1.conv2\n",
      "bias: False\n",
      "Original Shape: torch.Size([256, 256, 3, 3])\n",
      "Flattened Shape: torch.Size([256, 256, 9])\n",
      "Input shape: torch.Size([1, 256, 14, 14])\n",
      "Output shape: torch.Size([1, 256, 14, 14])\n",
      "Parameters: 589824\n",
      "Original MACs: 115605504\n",
      "Reduction rates: [1, 1.5, 2]\n",
      "Reduction ranks: [1132, 754, 566]\n",
      "Reduced MACs(%): [99.99, 66.6, 50.0]\n",
      "\n",
      "Layer: layer4.0.conv1\n",
      "bias: False\n",
      "Original Shape: torch.Size([512, 256, 3, 3])\n",
      "Flattened Shape: torch.Size([512, 256, 9])\n",
      "Input shape: torch.Size([1, 256, 14, 14])\n",
      "Output shape: torch.Size([1, 512, 7, 7])\n",
      "Parameters: 1179648\n",
      "Original MACs: 57802752\n",
      "Reduction rates: [1, 1.5, 2]\n",
      "Reduction ranks: [1518, 1012, 759]\n",
      "Reduced MACs(%): [198.81, 132.54, 99.41]\n",
      "\n",
      "Layer: layer4.0.conv2\n",
      "bias: False\n",
      "Original Shape: torch.Size([512, 512, 3, 3])\n",
      "Flattened Shape: torch.Size([512, 512, 9])\n",
      "Input shape: torch.Size([1, 512, 7, 7])\n",
      "Output shape: torch.Size([1, 512, 7, 7])\n",
      "Parameters: 2359296\n",
      "Original MACs: 115605504\n",
      "Reduction rates: [1, 1.5, 2]\n",
      "Reduction ranks: [2283, 1522, 1141]\n",
      "Reduced MACs(%): [99.96, 66.64, 49.96]\n",
      "\n",
      "Layer: layer4.0.downsample\n",
      "bias: False\n",
      "Original Shape: torch.Size([512, 256, 1, 1])\n",
      "Flattened Shape: torch.Size([512, 256])\n",
      "Input shape: torch.Size([1, 256, 14, 14])\n",
      "Output shape: torch.Size([1, 512, 7, 7])\n",
      "Parameters: 131072\n",
      "Original MACs: 6422528\n",
      "Reduction rates: [1, 1.5, 2]\n",
      "Reduction ranks: [170, 113, 85]\n",
      "\n",
      "Layer: layer4.1.conv1\n",
      "bias: False\n",
      "Original Shape: torch.Size([512, 512, 3, 3])\n",
      "Flattened Shape: torch.Size([512, 512, 9])\n",
      "Input shape: torch.Size([1, 512, 7, 7])\n",
      "Output shape: torch.Size([1, 512, 7, 7])\n",
      "Parameters: 2359296\n",
      "Original MACs: 115605504\n",
      "Reduction rates: [1, 1.5, 2]\n",
      "Reduction ranks: [2283, 1522, 1141]\n",
      "Reduced MACs(%): [99.96, 66.64, 49.96]\n",
      "\n",
      "Layer: layer4.1.conv2\n",
      "bias: False\n",
      "Original Shape: torch.Size([512, 512, 3, 3])\n",
      "Flattened Shape: torch.Size([512, 512, 9])\n",
      "Input shape: torch.Size([1, 512, 7, 7])\n",
      "Output shape: torch.Size([1, 512, 7, 7])\n",
      "Parameters: 2359296\n",
      "Original MACs: 115605504\n",
      "Reduction rates: [1, 1.5, 2]\n",
      "Reduction ranks: [2283, 1522, 1141]\n",
      "Reduced MACs(%): [99.96, 66.64, 49.96]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# REDUCTION_RATES = [1, 2, 4, 5, 6, 7, 8]\n",
    "REDUCTION_RATES = [1, 1.5, 2]\n",
    "rank_map = {}\n",
    "dummy_input = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "# original macs = C_i * W_k * H_k * C_o * W_o * H_o\n",
    "# reduced macs = rank * C_i * W_i * H_i + rank * W_k * H_k * W_o * H_o + rank * C_o * W_o * H_o\n",
    "layer = orig_model.conv1\n",
    "weight = orig_model.conv1.weight.detach()\n",
    "bias = orig_model.conv1.bias\n",
    "with torch.no_grad(): y = layer(dummy_input)\n",
    "print('Layer: conv1')\n",
    "print('bias:', bias is not None)\n",
    "print('Original Shape:', weight.shape)\n",
    "weight = weight.reshape((weight.shape[0], weight.shape[1], -1))\n",
    "print('Flattened Shape:', weight.shape)\n",
    "print('Input shape:', dummy_input.shape)\n",
    "print()\n",
    "\n",
    "print('Parameters:', weight.numel())\n",
    "orig_macs = torch.tensor(weight.shape).prod() * y.shape[-1] * y.shape[-2]\n",
    "print('Original MACs:', orig_macs.item())\n",
    "# print('Feasible ranks <', X.numel()/sum(list(X.shape))/REDUCTION_RATE)\n",
    "ranks = list(int(weight.numel()/sum(list(weight.shape))/rate) for rate in REDUCTION_RATES)\n",
    "print('Reduction rates:', REDUCTION_RATES)\n",
    "print('Reduction ranks:', ranks)\n",
    "print()\n",
    "rank_map['conv1'] = ranks\n",
    "y = orig_model.maxpool(y)\n",
    "\n",
    "for layer_name in ['layer1.0.conv1', 'layer1.0.conv2', 'layer1.1.conv1', 'layer1.1.conv2', \n",
    "                   'layer2.0.conv1', 'layer2.0.conv2', 'layer2.0.downsample', 'layer2.1.conv1', 'layer2.1.conv2',\n",
    "                   'layer3.0.conv1', 'layer3.0.conv2', 'layer3.0.downsample', 'layer3.1.conv1', 'layer3.1.conv2',\n",
    "                   'layer4.0.conv1', 'layer4.0.conv2', 'layer4.0.downsample', 'layer4.1.conv1', 'layer4.1.conv2']:\n",
    "    lname, lidx, ltype = layer_name.split('.')\n",
    "    lidx = int(lidx)\n",
    "    layer = orig_model.__getattr__(lname)[lidx].__getattr__(ltype)\n",
    "    if ltype == 'downsample': layer = layer[0]\n",
    "    weight = layer.weight.detach()\n",
    "    bias = orig_model.conv1.bias\n",
    "    print('Layer:', layer_name)\n",
    "    print('bias:', bias is not None)\n",
    "    print('Original Shape:', weight.shape)\n",
    "    weight = weight.reshape((weight.shape[0], weight.shape[1], -1))\n",
    "    if ltype == 'downsample': weight = weight.reshape((weight.shape[0], weight.shape[1]))\n",
    "    print('Flattened Shape:', weight.shape)\n",
    "    \n",
    "    c1 = layer.in_channels * y.shape[-1] * y.shape[-2]\n",
    "    if layer_name.endswith('.0.conv1'): \n",
    "        print('Input shape:', y.shape)\n",
    "        with torch.no_grad(): y = layer(y)\n",
    "        print('Output shape:', y.shape)\n",
    "    elif ltype == 'downsample':\n",
    "        print('Input shape:', res.shape)\n",
    "        with torch.no_grad(): print('Output shape:', layer(res).shape)\n",
    "    else:\n",
    "        print('Input shape:', y.shape)\n",
    "        print('Output shape:', y.shape)\n",
    "    c2 = layer.kernel_size[0] * layer.kernel_size[1] * y.shape[-1] * y.shape[-2]\n",
    "    c3 = layer.out_channels * y.shape[-1] * y.shape[-2]\n",
    "    if layer_name.endswith('.1.conv2'):\n",
    "        res = y\n",
    "    \n",
    "    print('Parameters:', weight.numel())\n",
    "    orig_macs = torch.tensor(weight.shape).prod() * y.shape[-1] * y.shape[-2]\n",
    "    print('Original MACs:', orig_macs.item())\n",
    "    ranks = list(int(weight.numel()/sum(list(weight.shape))/rate) for rate in REDUCTION_RATES)\n",
    "    print('Reduction rates:', REDUCTION_RATES)\n",
    "    print('Reduction ranks:', ranks)\n",
    "    if ltype != 'downsample': \n",
    "        macs = [(rank * (c1 + c2 + c3) * 100/orig_macs).item() for rank in ranks]\n",
    "        print('Reduced MACs(%):', [round(mac, 2) for mac in macs])\n",
    "    print()\n",
    "    rank_map[layer_name] = ranks\n",
    "    \n",
    "\n",
    "# X = orig_model.fc.weight.detach()\n",
    "# bias = orig_model.conv1.bias\n",
    "# print('Layer: fc')\n",
    "# print('Original Shape:', X.shape)\n",
    "# print('Parameters:', X.numel())\n",
    "# print('bias:', bias is not None)\n",
    "# # print('Feasible rank < ', X.numel()/sum(list(X.shape))/rate)\n",
    "# ranks = list(int(X.numel()/sum(list(X.shape))/rate) for rate in REDUCTION_RATES)\n",
    "# print('Feasible ranks = ', ranks)\n",
    "# rank_map['fc'] = ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dfb7975-0ce6-473e-b6d3-f29f90af446d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv1': [81, 54, 40],\n",
       " 'layer1.0.conv1': [269, 179, 134],\n",
       " 'layer1.0.conv2': [269, 179, 134],\n",
       " 'layer1.1.conv1': [269, 179, 134],\n",
       " 'layer1.1.conv2': [269, 179, 134],\n",
       " 'layer2.0.conv1': [366, 244, 183],\n",
       " 'layer2.0.conv2': [556, 370, 278],\n",
       " 'layer2.0.downsample': [42, 28, 21],\n",
       " 'layer2.1.conv1': [556, 370, 278],\n",
       " 'layer2.1.conv2': [556, 370, 278],\n",
       " 'layer3.0.conv1': [750, 500, 375],\n",
       " 'layer3.0.conv2': [1132, 754, 566],\n",
       " 'layer3.0.downsample': [85, 56, 42],\n",
       " 'layer3.1.conv1': [1132, 754, 566],\n",
       " 'layer3.1.conv2': [1132, 754, 566],\n",
       " 'layer4.0.conv1': [1518, 1012, 759],\n",
       " 'layer4.0.conv2': [2283, 1522, 1141],\n",
       " 'layer4.0.downsample': [170, 113, 85],\n",
       " 'layer4.1.conv1': [2283, 1522, 1141],\n",
       " 'layer4.1.conv2': [2283, 1522, 1141]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00a69b3b-2594-4d95-ae04-65faa9393a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv1': 40,\n",
       " 'layer1.0.conv1': 134,\n",
       " 'layer1.0.conv2': 134,\n",
       " 'layer1.1.conv1': 134,\n",
       " 'layer1.1.conv2': 134,\n",
       " 'layer2.0.conv1': 183,\n",
       " 'layer2.0.conv2': 278,\n",
       " 'layer2.0.downsample': 21,\n",
       " 'layer2.1.conv1': 278,\n",
       " 'layer2.1.conv2': 278,\n",
       " 'layer3.0.conv1': 375,\n",
       " 'layer3.0.conv2': 566,\n",
       " 'layer3.0.downsample': 42,\n",
       " 'layer3.1.conv1': 566,\n",
       " 'layer3.1.conv2': 566,\n",
       " 'layer4.0.conv1': 759,\n",
       " 'layer4.0.conv2': 1141,\n",
       " 'layer4.0.downsample': 85,\n",
       " 'layer4.1.conv1': 1141,\n",
       " 'layer4.1.conv2': 1141}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:v[2] for (k,v) in rank_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e10ebf7-8f06-4e57-a140-67dba86fabf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mark23",
   "language": "python",
   "name": "mark23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
