{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5219c3d5-ed3d-40cc-874a-14e2f6b4095f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "147761c9-1a1b-44c4-8885-c3c67ee262ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/trinity/home/daria.cherniuk/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /trinity/home/daria.cherniuk/.local/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/trinity/home/daria.cherniuk/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/trinity/home/daria.cherniuk/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/trinity/home/daria.cherniuk/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/.singularity.d/env/91-environment.sh')}\n",
      "  warn(msg)\n",
      "/trinity/home/daria.cherniuk/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  eval `/usr/bin/modulecmd bash $*`\\n}')}\n",
      "  warn(msg)\n",
      "/trinity/home/daria.cherniuk/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/etc/modulefiles'), PosixPath('/usr/share/Modules/modulefiles')}\n",
      "  warn(msg)\n",
      "/trinity/home/daria.cherniuk/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/share/Modules')}\n",
      "  warn(msg)\n",
      "/trinity/home/daria.cherniuk/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/lib64/qt-3.3')}\n",
      "  warn(msg)\n",
      "/trinity/home/daria.cherniuk/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/lib64/qt-3.3/include')}\n",
      "  warn(msg)\n",
      "/trinity/home/daria.cherniuk/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/lib64/qt-3.3/lib')}\n",
      "  warn(msg)\n",
      "/trinity/home/daria.cherniuk/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/etc/slurm/slurm.conf')}\n",
      "  warn(msg)\n",
      "/trinity/home/daria.cherniuk/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/beegfs/home/daria.cherniuk')}\n",
      "  warn(msg)\n",
      "/trinity/home/daria.cherniuk/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/libexec/openssh/gnome-ssh-askpass')}\n",
      "  warn(msg)\n",
      "/trinity/home/daria.cherniuk/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/trinity/home/daria.cherniuk/bin'), PosixPath('/opt/bin'), PosixPath('/usr/lib64/qt-3.3/bin')}\n",
      "  warn(msg)\n",
      "/trinity/home/daria.cherniuk/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/trinity/home/daria.cherniuk/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 111\n",
      "CUDA SETUP: Loading binary /trinity/home/daria.cherniuk/.local/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "from source.quantization import quantize_tensor\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1656e3b-2a58-4a2c-aa0e-698a98f56cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(bits, model):\n",
    "    cls = bnb.nn.Linear4bit if bits == 4 else (bnb.nn.Linear8bitLt if bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57e1ef1e-fb89-4b69-acbf-5c4d6091126f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3300e79f-9650-4c34-9328-b11e8916659e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:48<00:00, 24.05s/it]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"huggyllama/llama-7b\"\n",
    "cache_dir='/gpfs/gpfs0/daria.cherniuk/huggycache/'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                             # quantization_config=bnb_config, \n",
    "                                             device_map={\"\":0},\n",
    "                                             cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "685f4c21-eb37-4bb8-ad84-a943e47a04bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40929408-dbcd-4a14-9bab-316752c09323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug  5 00:29:40 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  Off  | 00000000:27:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    70W / 400W |  26615MiB / 81920MiB |      1%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1759353      C   /usr/bin/python3                26613MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc6af7d5-c7bd-4fc9-a26e-bef784f5ce02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9afda02f-5d2a-4b55-98d4-479239e3b721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb6be8b5-8373-4a6d-b99d-8e074748b317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, True, True, True, True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "f'model.layers.{i}.self_attn.q_proj.weight' in state_dict, \\\n",
    "f'model.layers.{i}.self_attn.k_proj.weight' in state_dict, \\\n",
    "f'model.layers.{i}.self_attn.v_proj.weight' in state_dict, \\\n",
    "f'model.layers.{i}.self_attn.o_proj.weight' in state_dict, \\\n",
    "f'model.layers.{i}.mlp.up_proj.weight' in state_dict, \\\n",
    "f'model.layers.{i}.mlp.gate_proj.weight' in state_dict, \\\n",
    "f'model.layers.{i}.mlp.down_proj.weight' in state_dict,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b53e566-8e7f-4f1f-9582-1d0573e2d80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 4096])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1a0fb846-152f-4b52-b553-131026440216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['q_proj', 'o_proj', 'up_proj', 'v_proj', 'gate_proj', 'down_proj', 'k_proj']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modules = find_all_linear_names(bits=4, model=model)\n",
    "modules = find_all_linear_names(bits=32, model=model)\n",
    "modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "80f0b947-f4a1-4f14-98e8-94208011bcd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 4096])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in_features = model.model.layers[0].self_attn.q_proj.in_features\n",
    "# out_features = model.model.layers[0].self_attn.q_proj.out_features\n",
    "# W = state_dict['model.layers.0.self_attn.q_proj.weight'].reshape(out_features, in_features)\n",
    "W = state_dict['model.layers.0.self_attn.q_proj.weight']\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3d4e3487-5f5a-4554-ba11-d5e86bedf166",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def project_rank(H, rank):\n",
    "    U, S, Vt = torch.linalg.svd(H)\n",
    "    return U[:,:rank] @ torch.diag(S[:rank]) @ Vt[:rank]\n",
    "\n",
    "def admm_iteration(H, U, W, H2, proj_func, rho=1.0, max_iter=50, eps=1e-8):\n",
    "    # rank = H.shape[1]\n",
    "    # rho = torch.trace(G) / rank\n",
    "    # rho = 1.0\n",
    "    for j in range(1, max_iter):\n",
    "        H_ = (rho*(H + U) + W - H2) / (1 + rho)\n",
    "        H_prev = H.clone()\n",
    "        # H = quantize_tensor(H_T - U, qscheme=qscheme, bits=bits)\n",
    "        H = proj_func(H_ - U)\n",
    "        U += H - H_\n",
    "\n",
    "        r = torch.sum((H - H_)**2) / torch.sum(H**2)\n",
    "        s = torch.sum((H - H_prev)**2) / torch.sum(U**2) \n",
    "        if r < eps and s < eps: \n",
    "            break\n",
    "            \n",
    "    return H, U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d293362f-b620-4d0e-b549-87f0a1f65945",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits: 4, Rank: 4\n",
      "Diff between W and quantized W abs: 122.0862, rel: 0.9553\n",
      "Diff between W and (W_q + W_r) abs: 4106.7480, rel: 32.1340\n",
      "Diff between W and (W_q + W_r) abs: 129.7543, rel: 1.0153\n",
      "Diff between W and (W_q + W_r) abs: 122.7363, rel: 0.9604\n",
      "Diff between W and (W_q + W_r) abs: 123.5316, rel: 0.9666\n",
      "Diff between W and (W_q + W_r) abs: 120.9213, rel: 0.9462\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:31\u001b[0m\n",
      "Cell \u001b[0;32mIn[52], line 13\u001b[0m, in \u001b[0;36madmm_iteration\u001b[0;34m(H, U, W, H2, proj_func, rho, max_iter, eps)\u001b[0m\n\u001b[1;32m     11\u001b[0m H_prev \u001b[38;5;241m=\u001b[39m H\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# H = quantize_tensor(H_T - U, qscheme=qscheme, bits=bits)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m H \u001b[38;5;241m=\u001b[39m \u001b[43mproj_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mU\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m U \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m H \u001b[38;5;241m-\u001b[39m H_\n\u001b[1;32m     16\u001b[0m r \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum((H \u001b[38;5;241m-\u001b[39m H_)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(H\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[52], line 2\u001b[0m, in \u001b[0;36mproject_rank\u001b[0;34m(H, rank)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mproject_rank\u001b[39m(H, rank):\n\u001b[0;32m----> 2\u001b[0m     U, S, Vt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m U[:,:rank] \u001b[38;5;241m@\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag(S[:rank]) \u001b[38;5;241m@\u001b[39m Vt[:rank]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# projection functions\n",
    "bits = 4\n",
    "rank = 4\n",
    "init = 'random'\n",
    "quantize_func = partial(quantize_tensor, qscheme='tensor_minmax', bits=bits)\n",
    "project_func = partial(project_rank, rank=rank)\n",
    "\n",
    "# original matrix\n",
    "W = state_dict['model.layers.0.self_attn.q_proj.weight']\n",
    "\n",
    "# init W_q as quantized W or as random matrix\n",
    "if init == 'random':\n",
    "    W_q = torch.randn(*W.shape, device=device)\n",
    "else:\n",
    "    W_q = quantize_func(W)\n",
    "U_q = torch.zeros_like(W_q, device=device)\n",
    "\n",
    "# init W_r as low-rank random matrix\n",
    "W_r = torch.randn(*W.shape, device=device)\n",
    "W_r = project_func(W_r)\n",
    "U_r = torch.zeros_like(W_r, device=device)\n",
    "\n",
    "print(f'Bits: {bits}, Rank: {rank}')\n",
    "print(f'Diff between W and quantized W abs: {torch.linalg.norm(W - quantize_func(W)):.4f}, rel: {torch.linalg.norm(W - quantize_func(W))/torch.linalg.norm(W):.4f}')\n",
    "print(f'Diff between W and (W_q + W_r) abs: {torch.linalg.norm(W - W_r - W_q):.4f}, rel: {torch.linalg.norm(W - W_r - W_q)/torch.linalg.norm(W):.4f}')\n",
    "\n",
    "# ADMM iteration\n",
    "rho = 1.0\n",
    "for i in range(500):\n",
    "    W_q, U_q = admm_iteration(W_q, U_q, W, W_r, quantize_func, rho=rho)\n",
    "    W_r, U_r = admm_iteration(W_r, U_r, W, W_q, project_func, rho=rho)\n",
    "    if i % 10 == 0:\n",
    "        print(f'Diff between W and (W_q + W_r) abs: {torch.linalg.norm(W - W_r - W_q):.4f}, rel: {torch.linalg.norm(W - W_r - W_q)/torch.linalg.norm(W):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e995c5d7-d962-4875-817e-050c30ddca31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits: 4, Rank: 8\n",
      "Diff between W and quantized W abs: 122.0862, rel: 0.9553\n",
      "Diff between W and (W_q + W_r) abs: 4114.1504, rel: 32.1919\n",
      "Diff between W and (W_q + W_r) abs: 131.3533, rel: 1.0278\n",
      "Diff between W and (W_q + W_r) abs: 131.2432, rel: 1.0269\n",
      "Diff between W and (W_q + W_r) abs: 131.4433, rel: 1.0285\n",
      "Diff between W and (W_q + W_r) abs: 131.2081, rel: 1.0267\n",
      "Diff between W and (W_q + W_r) abs: 130.6566, rel: 1.0223\n",
      "Diff between W and (W_q + W_r) abs: 130.7635, rel: 1.0232\n",
      "Diff between W and (W_q + W_r) abs: 129.7207, rel: 1.0150\n",
      "Diff between W and (W_q + W_r) abs: 129.1010, rel: 1.0102\n",
      "Diff between W and (W_q + W_r) abs: 128.7581, rel: 1.0075\n",
      "Diff between W and (W_q + W_r) abs: 128.8057, rel: 1.0079\n",
      "CPU times: user 2h 32min 12s, sys: 167 ms, total: 2h 32min 12s\n",
      "Wall time: 2h 32min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# projection functions\n",
    "bits = 4\n",
    "rank = 8\n",
    "init = 'random'\n",
    "quantize_func = partial(quantize_tensor, qscheme='tensor_minmax', bits=bits)\n",
    "project_func = partial(project_rank, rank=rank)\n",
    "\n",
    "# original matrix\n",
    "W = state_dict['model.layers.0.self_attn.q_proj.weight']\n",
    "\n",
    "# init W_q as quantized W or as random matrix\n",
    "if init == 'random':\n",
    "    W_q = torch.randn(*W.shape, device=device)\n",
    "else:\n",
    "    W_q = quantize_func(W)\n",
    "U_q = torch.zeros_like(W_q, device=device)\n",
    "\n",
    "# init W_r as low-rank random matrix\n",
    "W_r = torch.randn(*W.shape, device=device)\n",
    "W_r = project_func(W_r)\n",
    "U_r = torch.zeros_like(W_r, device=device)\n",
    "\n",
    "print(f'Bits: {bits}, Rank: {rank}')\n",
    "print(f'Diff between W and quantized W abs: {torch.linalg.norm(W - quantize_func(W)):.4f}, rel: {torch.linalg.norm(W - quantize_func(W))/torch.linalg.norm(W):.4f}')\n",
    "print(f'Diff between W and (W_q + W_r) abs: {torch.linalg.norm(W - W_r - W_q):.4f}, rel: {torch.linalg.norm(W - W_r - W_q)/torch.linalg.norm(W):.4f}')\n",
    "\n",
    "# ADMM iteration\n",
    "rho = 1.0\n",
    "for i in range(100):\n",
    "    W_q, U_q = admm_iteration(W_q, U_q, W, W_r, quantize_func, rho=rho)\n",
    "    W_r, U_r = admm_iteration(W_r, U_r, W, W_q, project_func, rho=rho)\n",
    "    if i % 10 == 0:\n",
    "        print(f'Diff between W and (W_q + W_r) abs: {torch.linalg.norm(W - W_r - W_q):.4f}, rel: {torch.linalg.norm(W - W_r - W_q)/torch.linalg.norm(W):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a30308e0-dce9-4868-b37f-35e5d4404e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits: 2, Rank: 4\n",
      "Diff between W and quantized W abs: 929.1003, rel: 7.2699\n",
      "Diff between W and (W_q + W_r) abs: 4106.0610, rel: 32.1286\n",
      "Diff between W and (W_q + W_r) abs: 705.8522, rel: 5.5231\n",
      "Diff between W and (W_q + W_r) abs: 503.7399, rel: 3.9416\n",
      "Diff between W and (W_q + W_r) abs: 495.2279, rel: 3.8750\n",
      "Diff between W and (W_q + W_r) abs: 765.5813, rel: 5.9904\n",
      "Diff between W and (W_q + W_r) abs: 973.9545, rel: 7.6209\n",
      "Diff between W and (W_q + W_r) abs: 1115.0795, rel: 8.7251\n",
      "Diff between W and (W_q + W_r) abs: 1219.1031, rel: 9.5391\n",
      "Diff between W and (W_q + W_r) abs: 1297.3093, rel: 10.1510\n",
      "Diff between W and (W_q + W_r) abs: 1357.7460, rel: 10.6239\n",
      "Diff between W and (W_q + W_r) abs: 1412.9738, rel: 11.0561\n",
      "CPU times: user 2h 45min 46s, sys: 1.24 s, total: 2h 45min 47s\n",
      "Wall time: 2h 46min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# projection functions\n",
    "bits = 2\n",
    "rank = 4\n",
    "init = 'random'\n",
    "quantize_func = partial(quantize_tensor, qscheme='tensor_minmax', bits=bits)\n",
    "project_func = partial(project_rank, rank=rank)\n",
    "\n",
    "# original matrix\n",
    "W = state_dict['model.layers.0.self_attn.q_proj.weight']\n",
    "\n",
    "# init W_q as quantized W or as random matrix\n",
    "if init == 'random':\n",
    "    W_q = torch.randn(*W.shape, device=device)\n",
    "else:\n",
    "    W_q = quantize_func(W)\n",
    "U_q = torch.zeros_like(W_q, device=device)\n",
    "\n",
    "# init W_r as low-rank random matrix\n",
    "W_r = torch.randn(*W.shape, device=device)\n",
    "W_r = project_func(W_r)\n",
    "U_r = torch.zeros_like(W_r, device=device)\n",
    "\n",
    "print(f'Bits: {bits}, Rank: {rank}')\n",
    "print(f'Diff between W and quantized W abs: {torch.linalg.norm(W - quantize_func(W)):.4f}, rel: {torch.linalg.norm(W - quantize_func(W))/torch.linalg.norm(W):.4f}')\n",
    "print(f'Diff between W and (W_q + W_r) abs: {torch.linalg.norm(W - W_r - W_q):.4f}, rel: {torch.linalg.norm(W - W_r - W_q)/torch.linalg.norm(W):.4f}')\n",
    "\n",
    "# ADMM iteration\n",
    "rho = 1.0\n",
    "for i in range(100):\n",
    "    W_q, U_q = admm_iteration(W_q, U_q, W, W_r, quantize_func, rho=rho)\n",
    "    W_r, U_r = admm_iteration(W_r, U_r, W, W_q, project_func, rho=rho)\n",
    "    if i % 10 == 0:\n",
    "        print(f'Diff between W and (W_q + W_r) abs: {torch.linalg.norm(W - W_r - W_q):.4f}, rel: {torch.linalg.norm(W - W_r - W_q)/torch.linalg.norm(W):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050e6a6a-22b0-4f89-8150-28ead99ca994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
